{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff6e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ujson as json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "CHUNK_SIZE = 50\n",
    "MAX_SAMPLES = 500\n",
    "MAX_CHARS = 512\n",
    "OUTPUT_PATH = \"pretrain_smollm.jsonl\"\n",
    "\n",
    "def write_chunk(chunk, file_path):\n",
    "    \"\"\"Shuffle and write chunk to disk.\"\"\"\n",
    "    random.shuffle(chunk)\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for item in chunk:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"ðŸ”„ Streaming smollm / cosmopedia-v2 dataset...\")\n",
    "\n",
    "    dataset = load_dataset(\n",
    "        \"HuggingFaceTB/smollm-corpus\",\n",
    "        \"cosmopedia-v2\",\n",
    "        split=\"train\",\n",
    "        streaming=True\n",
    "    )\n",
    "\n",
    "    buffer = []\n",
    "    count = 0\n",
    "    chunk_id = 1\n",
    "\n",
    "    total_tokens = 0\n",
    "\n",
    "    for sample in dataset:\n",
    "\n",
    "        if count >= MAX_SAMPLES:\n",
    "            break\n",
    "\n",
    "        text = (sample.get(\"text\") or \"\").strip()\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # STRICT CHAR LIMIT\n",
    "        if len(text) >= MAX_CHARS:\n",
    "            continue\n",
    "\n",
    "        token_count = text.count(\" \") + 1\n",
    "        total_tokens += token_count\n",
    "\n",
    "        formatted = {\n",
    "            \"text\": f\"<|im_start|>{text}<|im_end|>\"\n",
    "        }\n",
    "\n",
    "        buffer.append(formatted)\n",
    "        count += 1\n",
    "\n",
    "        if len(buffer) >= CHUNK_SIZE:\n",
    "            random.shuffle(buffer)\n",
    "            print(f\"ðŸ§¹ Writing chunk {chunk_id} ({len(buffer)} samples)... Total so far: {count}\")\n",
    "            write_chunk(buffer, OUTPUT_PATH)\n",
    "            buffer = []\n",
    "            chunk_id += 1\n",
    "\n",
    "    if buffer:\n",
    "        print(f\"ðŸ§¹ Writing final chunk ({len(buffer)} samples)...\")\n",
    "        write_chunk(buffer, OUTPUT_PATH)\n",
    "\n",
    "    # ---------------- SUMMARY ----------------\n",
    "    print(\"\\nðŸŽ‰ DONE!\")\n",
    "    print(f\"ðŸ“„ Saved to: {OUTPUT_PATH}\")\n",
    "    print(f\"ðŸ“¦ Total samples written: {count}\")\n",
    "    print(f\"ðŸ”¢ Total tokens (approx): {total_tokens:,}\")\n",
    "    print(f\"ðŸ”¢ Total tokens in millions: {total_tokens / 1e6:.3f}M\")\n",
    "    print(f\"ðŸ”¢ Total tokens in billions: {total_tokens / 1e9:.4f}B\")\n",
    "    print(f\"ðŸ§® Avg tokens per sample: {total_tokens / count:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
