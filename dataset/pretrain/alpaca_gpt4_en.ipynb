{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1df18bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Loading dataset: vicgalle/alpaca-gpt4 (streaming mode)\n",
      "ğŸ“ Writing chunk... 5000 samples so far\n",
      "ğŸ“ Writing chunk... 10000 samples so far\n",
      "ğŸ“ Writing chunk... 15000 samples so far\n",
      "ğŸ“ Writing chunk... 20000 samples so far\n",
      "\n",
      "ğŸ‰ DONE!\n",
      "ğŸ“¦ Total samples written: 23093\n",
      "ğŸ”¤ Total characters: 6,151,513\n",
      "ğŸ”¢ Estimated tokens: 1,537,878\n",
      "ğŸ”¢ Tokens (millions): 1.538M\n",
      "ğŸ”¢ Tokens (billions): 0.0015B\n",
      "ğŸ“„ Saved to: alpaca_gpt4_en_pretrain.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "DATASET_ID = \"vicgalle/alpaca-gpt4\"\n",
    "SPLIT = \"train\"\n",
    "OUTPUT_PATH = \"alpaca_gpt4_en_pretrain.jsonl\"\n",
    "\n",
    "MAX_SAMPLES = 200_000\n",
    "MAX_CHARS = 512\n",
    "CHUNK_SIZE = 5000\n",
    "\n",
    "BOS = \"<|im_start|>\"\n",
    "EOS = \"<|im_end|>\"\n",
    "# ============================================================\n",
    "\n",
    "\n",
    "def safe_get(d, keys):\n",
    "    \"\"\"Return the first available key from list, or '' \"\"\"\n",
    "    for k in keys:\n",
    "        if k in d and d[k] not in [None, \"\"]:\n",
    "            return str(d[k])\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def build_prompt(sample):\n",
    "    \"\"\"\n",
    "    Build a unified prompt for any dataset format.\n",
    "    Supports:\n",
    "      - instruction + input + output\n",
    "      - instruction + output\n",
    "      - text-only datasets\n",
    "    \"\"\"\n",
    "\n",
    "    instr = safe_get(sample, [\"instruction\", \"prompt\", \"question\"])\n",
    "    inp = safe_get(sample, [\"input\", \"context\"])\n",
    "    out = safe_get(sample, [\"output\", \"answer\", \"completion\"])\n",
    "    text = safe_get(sample, [\"text\"])\n",
    "\n",
    "    # Raw text-only dataset (pretraining)\n",
    "    if instr == \"\" and out == \"\" and text != \"\":\n",
    "        return f\"{BOS}{text}{EOS}\"\n",
    "\n",
    "    # Build user message\n",
    "    user_msg = instr\n",
    "    if inp != \"\":\n",
    "        user_msg += \"\\n\" + inp\n",
    "\n",
    "    # No output â†’ treat as single-turn text\n",
    "    if out == \"\":\n",
    "        return f\"{BOS}{user_msg}{EOS}\"\n",
    "\n",
    "    # Instruction dataset\n",
    "    return (\n",
    "        f\"{BOS}user\\n{user_msg}{EOS}\\n\"\n",
    "        f\"{BOS}assistant\\n{out}{EOS}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def write_chunk(buffer, path):\n",
    "    random.shuffle(buffer)\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for x in buffer:\n",
    "            f.write(json.dumps(x, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    print(f\"ğŸš€ Loading dataset: {DATASET_ID} (streaming mode)\")\n",
    "    dataset = load_dataset(DATASET_ID, split=SPLIT, streaming=True)\n",
    "\n",
    "    buffer = []\n",
    "    written = 0\n",
    "    total_chars = 0  # <-- track total characters\n",
    "\n",
    "    for sample in dataset:\n",
    "\n",
    "        if written >= MAX_SAMPLES:\n",
    "            break\n",
    "\n",
    "        prompt = build_prompt(sample)\n",
    "\n",
    "        # Character-level filtering only\n",
    "        if len(prompt) > MAX_CHARS:\n",
    "            continue\n",
    "\n",
    "        total_chars += len(prompt)\n",
    "\n",
    "        buffer.append({\"text\": prompt})\n",
    "        written += 1\n",
    "\n",
    "        if len(buffer) >= CHUNK_SIZE:\n",
    "            print(f\"ğŸ“ Writing chunk... {written} samples so far\")\n",
    "            write_chunk(buffer, OUTPUT_PATH)\n",
    "            buffer = []\n",
    "\n",
    "    # final flush\n",
    "    if buffer:\n",
    "        write_chunk(buffer, OUTPUT_PATH)\n",
    "\n",
    "    # final stats\n",
    "    est_tokens = total_chars / 4  # approx 4 chars/token\n",
    "\n",
    "    print(\"\\nğŸ‰ DONE!\")\n",
    "    print(f\"ğŸ“¦ Total samples written: {written}\")\n",
    "    print(f\"ğŸ”¤ Total characters: {total_chars:,}\")\n",
    "    print(f\"ğŸ”¢ Estimated tokens: {est_tokens:,.0f}\")\n",
    "    print(f\"ğŸ”¢ Tokens (millions): {est_tokens / 1e6:.3f}M\")\n",
    "    print(f\"ğŸ”¢ Tokens (billions): {est_tokens / 1e9:.4f}B\")\n",
    "    print(f\"ğŸ“„ Saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
