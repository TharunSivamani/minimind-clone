{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3774b5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Œ Shard: train-00000-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40885\n",
      "  Ratio: 0.1087\n",
      "\n",
      "ðŸ“Œ Shard: train-00001-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 41026\n",
      "  Ratio: 0.1090\n",
      "\n",
      "ðŸ“Œ Shard: train-00002-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40624\n",
      "  Ratio: 0.1080\n",
      "\n",
      "ðŸ“Œ Shard: train-00003-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40874\n",
      "  Ratio: 0.1086\n",
      "\n",
      "ðŸ“Œ Shard: train-00004-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 41324\n",
      "  Ratio: 0.1098\n",
      "\n",
      "ðŸ“Œ Shard: train-00005-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40575\n",
      "  Ratio: 0.1078\n",
      "\n",
      "ðŸ“Œ Shard: train-00006-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40704\n",
      "  Ratio: 0.1082\n",
      "\n",
      "ðŸ“Œ Shard: train-00007-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40797\n",
      "  Ratio: 0.1084\n",
      "\n",
      "ðŸ“Œ Shard: train-00008-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40728\n",
      "  Ratio: 0.1082\n",
      "\n",
      "ðŸ“Œ Shard: train-00009-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40884\n",
      "  Ratio: 0.1087\n",
      "\n",
      "ðŸ“Œ Shard: train-00010-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40768\n",
      "  Ratio: 0.1083\n",
      "\n",
      "ðŸ“Œ Shard: train-00011-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 40833\n",
      "  Ratio: 0.1085\n",
      "\n",
      "ðŸ“Œ Shard: train-00012-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 41093\n",
      "  Ratio: 0.1092\n",
      "\n",
      "ðŸ“Œ Shard: train-00013-of-00104.parquet\n",
      "  Total rows: 376289\n",
      "  Rows < 512 tokens: 41127\n",
      "  Ratio: 0.1093\n",
      "\n",
      "============ SUMMARY ============\n",
      "Total valid (<512 tokens) across all scanned shards: 572242\n",
      "Minimum no of < 512 samples in the corpus: 40575\n",
      "=================================\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.parquet as pq\n",
    "import pyarrow.compute as pc\n",
    "import math\n",
    "\n",
    "def count_short_sequences_arrow(path, max_len=512):\n",
    "    table = pq.read_table(path, columns=[\"token_length\"])\n",
    "    mask = pc.less(table[\"token_length\"], max_len)\n",
    "    count = pc.sum(mask).as_py()\n",
    "    total = table.num_rows\n",
    "    return count, total\n",
    "\n",
    "\n",
    "running_sum = 0\n",
    "minimum = 10000000\n",
    "\n",
    "for i in range(0, 14):  # 00000 to 00010\n",
    "    file = f\"train-{i:05d}-of-00104.parquet\"\n",
    "\n",
    "    try:\n",
    "        short, total = count_short_sequences_arrow(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âš ï¸ File not found: {file}, skippingâ€¦\")\n",
    "        continue\n",
    "\n",
    "    running_sum += short\n",
    "    minimum = min(minimum, short)\n",
    "\n",
    "    print(f\"\\nðŸ“Œ Shard: {file}\")\n",
    "    print(f\"  Total rows: {total}\")\n",
    "    print(f\"  Rows < 512 tokens: {short}\")\n",
    "    print(f\"  Ratio: {short / total:.4f}\")\n",
    "\n",
    "print(\"\\n============ SUMMARY ============\")\n",
    "print(f\"Total valid (<512 tokens) across all scanned shards: {running_sum}\")\n",
    "print(f\"Minimum no of < 512 samples in the corpus: {minimum}\")\n",
    "print(\"=================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01822a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Found 14 parquet files.\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00000-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 1 (40000) ... Total written: 40000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00001-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 2 (40000) ... Total written: 80000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00002-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 3 (40000) ... Total written: 120000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00003-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 4 (40000) ... Total written: 160000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00004-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 5 (40000) ... Total written: 200000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00005-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 6 (40000) ... Total written: 240000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00006-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 7 (40000) ... Total written: 280000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00007-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 8 (40000) ... Total written: 320000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00008-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 9 (40000) ... Total written: 360000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00009-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 10 (40000) ... Total written: 400000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00010-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 11 (40000) ... Total written: 440000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00011-of-00104.parquet\n",
      "ðŸ§¹ Writing chunk 12 (40000) ... Total written: 480000\n",
      "\n",
      "ðŸ”„ Processing file: ./train-00012-of-00104.parquet\n",
      "ðŸ§¹ Writing final chunk (20000)...\n",
      "\n",
      "ðŸŽ‰ DONE!\n",
      "ðŸ“¦ Total samples written: 500000\n",
      "ðŸ”¢ Total tokens: 209,720,958\n",
      "ðŸ“‰ Avg tokens per sample: 419.44\n",
      "ðŸ”¢ Tokens (millions): 209.721M\n",
      "ðŸ”¢ Tokens (billions): 0.2097B\n",
      "ðŸ“„ Saved to: pretrain_smollm.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "INPUT_DIR = \"./\"   # folder containing the parquet shards\n",
    "OUTPUT_JSONL = \"pretrain_smollm.jsonl\"\n",
    "\n",
    "CHUNK_SIZE = 40_000\n",
    "MAX_SAMPLES = 500_000\n",
    "MAX_TOKENS = 512\n",
    "\n",
    "\n",
    "def write_chunk(chunk, file_path):\n",
    "    \"\"\"Shuffle and write chunk to disk.\"\"\"\n",
    "    random.shuffle(chunk)\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        for row in chunk:\n",
    "            f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "def get_parquet_files(folder):\n",
    "    \"\"\"Auto-discover all train-* parquet files sorted numerically.\"\"\"\n",
    "    files = [f for f in os.listdir(folder) if f.startswith(\"train-\") and f.endswith(\".parquet\")]\n",
    "    files.sort()  # ensure 00000 â†’ 00001 â†’ ...\n",
    "    return [os.path.join(folder, f) for f in files]\n",
    "\n",
    "\n",
    "def main():\n",
    "    files = get_parquet_files(INPUT_DIR)\n",
    "    print(f\"ðŸ“‚ Found {len(files)} parquet files.\")\n",
    "\n",
    "    buffer = []\n",
    "    written = 0\n",
    "    total_tokens = 0\n",
    "    chunk_id = 1\n",
    "\n",
    "    for parquet_path in files:\n",
    "        if written >= MAX_SAMPLES:\n",
    "            break\n",
    "\n",
    "        print(f\"\\nðŸ”„ Processing file: {parquet_path}\")\n",
    "        pf = pq.ParquetFile(parquet_path)\n",
    "\n",
    "        for batch in pf.iter_batches(batch_size=10_000):\n",
    "            if written >= MAX_SAMPLES:\n",
    "                break\n",
    "\n",
    "            df = batch.to_pandas()\n",
    "            df = df[df[\"token_length\"] < MAX_TOKENS]\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                if written >= MAX_SAMPLES:\n",
    "                    break\n",
    "\n",
    "                text = row[\"text\"].strip()\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                total_tokens += row[\"token_length\"]\n",
    "\n",
    "                buffer.append({\n",
    "                    \"text\": f\"<|im_start|>{text}<|im_end|>\"\n",
    "                })\n",
    "                written += 1\n",
    "\n",
    "                if len(buffer) >= CHUNK_SIZE:\n",
    "                    print(f\"ðŸ§¹ Writing chunk {chunk_id} ({len(buffer)}) ... Total written: {written}\")\n",
    "                    write_chunk(buffer, OUTPUT_JSONL)\n",
    "                    buffer.clear()\n",
    "                    chunk_id += 1\n",
    "\n",
    "    # final flush\n",
    "    if buffer:\n",
    "        print(f\"ðŸ§¹ Writing final chunk ({len(buffer)})...\")\n",
    "        write_chunk(buffer, OUTPUT_JSONL)\n",
    "\n",
    "    print(\"\\nðŸŽ‰ DONE!\")\n",
    "    print(f\"ðŸ“¦ Total samples written: {written}\")\n",
    "    print(f\"ðŸ”¢ Total tokens: {total_tokens:,}\")\n",
    "    print(f\"ðŸ“‰ Avg tokens per sample: {total_tokens / written:.2f}\")\n",
    "    print(f\"ðŸ”¢ Tokens (millions): {total_tokens / 1e6:.3f}M\")\n",
    "    print(f\"ðŸ”¢ Tokens (billions): {total_tokens / 1e9:.4f}B\")\n",
    "    print(f\"ðŸ“„ Saved to: {OUTPUT_JSONL}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
