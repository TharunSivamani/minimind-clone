{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "147f6580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Scanning folder: final_dataset/\n",
      "üìÅ Found 6 JSONL files.\n",
      "\n",
      "üìÑ Processing file 1/6 ‚Üí pretrain_alpaca_gpt4_en.jsonl\n",
      "   üìå Entries in pretrain_alpaca_gpt4_en.jsonl: 23093\n",
      "\n",
      "üìÑ Processing file 2/6 ‚Üí pretrain_c4.jsonl\n",
      "   üìù Flushed 200000 entries (running total: 200000)\n",
      "   üìù Flushed 200000 entries (running total: 400000)\n",
      "   üìå Entries in pretrain_c4.jsonl: 500000\n",
      "\n",
      "üìÑ Processing file 3/6 ‚Üí pretrain_code_alpaca20k.jsonl\n",
      "   üìå Entries in pretrain_code_alpaca20k.jsonl: 16746\n",
      "\n",
      "üìÑ Processing file 4/6 ‚Üí pretrain_smollm.jsonl\n",
      "   üìù Flushed 200000 entries (running total: 600000)\n",
      "   üìù Flushed 200000 entries (running total: 800000)\n",
      "   üìù Flushed 200000 entries (running total: 1000000)\n",
      "   üìù Flushed 200000 entries (running total: 1200000)\n",
      "   üìù Flushed 200000 entries (running total: 1400000)\n",
      "   üìå Entries in pretrain_smollm.jsonl: 999978\n",
      "\n",
      "üìÑ Processing file 5/6 ‚Üí pretrain_stack_smol_code.jsonl\n",
      "   üìå Entries in pretrain_stack_smol_code.jsonl: 1428\n",
      "\n",
      "üìÑ Processing file 6/6 ‚Üí pretrain_wikipedia_en.jsonl\n",
      "   üìù Flushed 200000 entries (running total: 1600000)\n",
      "   üìù Flushed 200000 entries (running total: 1800000)\n",
      "   üìù Flushed 200000 entries (running total: 2000000)\n",
      "   üìå Entries in pretrain_wikipedia_en.jsonl: 500000\n",
      "\n",
      "üéâ MERGE COMPLETE!\n",
      "üì¶ Total entries written: 2041245\n",
      "üìÑ Output saved to: pretrain_en.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "INPUT_FOLDER = \"final_dataset/\"\n",
    "OUTPUT_FILE = \"pretrain_en.jsonl\"\n",
    "\n",
    "BUFFER_SIZE = 200_000   # controls shuffle quality vs memory use\n",
    "\n",
    "\n",
    "def merge_and_shuffle():\n",
    "    buffer = []\n",
    "    total_written = 0\n",
    "    file_count = 0\n",
    "\n",
    "    # Remove old output if exists\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        os.remove(OUTPUT_FILE)\n",
    "\n",
    "    print(\"üîç Scanning folder:\", INPUT_FOLDER)\n",
    "\n",
    "    jsonl_files = [f for f in os.listdir(INPUT_FOLDER) if f.endswith(\".jsonl\")]\n",
    "    jsonl_files.sort()  # optional\n",
    "\n",
    "    print(f\"üìÅ Found {len(jsonl_files)} JSONL files.\\n\")\n",
    "\n",
    "    for filename in jsonl_files:\n",
    "        path = os.path.join(INPUT_FOLDER, filename)\n",
    "        file_count += 1\n",
    "\n",
    "        print(f\"üìÑ Processing file {file_count}/{len(jsonl_files)} ‚Üí {filename}\")\n",
    "\n",
    "        file_entries = 0   # count entries for THIS file\n",
    "\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue  # skip invalid lines\n",
    "\n",
    "                buffer.append(entry)\n",
    "                file_entries += 1\n",
    "\n",
    "                # flush buffer if too large\n",
    "                if len(buffer) >= BUFFER_SIZE:\n",
    "                    random.shuffle(buffer)\n",
    "                    with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as out:\n",
    "                        for item in buffer:\n",
    "                            out.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "                    total_written += len(buffer)\n",
    "                    print(f\"   üìù Flushed {len(buffer)} entries (running total: {total_written})\")\n",
    "                    buffer = []\n",
    "\n",
    "        print(f\"   üìå Entries in {filename}: {file_entries}\\n\")\n",
    "\n",
    "    # flush remaining buffer\n",
    "    if buffer:\n",
    "        random.shuffle(buffer)\n",
    "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as out:\n",
    "            for item in buffer:\n",
    "                out.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        total_written += len(buffer)\n",
    "\n",
    "    print(\"üéâ MERGE COMPLETE!\")\n",
    "    print(f\"üì¶ Total entries written: {total_written}\")\n",
    "    print(f\"üìÑ Output saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "    return total_written\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merge_and_shuffle()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
